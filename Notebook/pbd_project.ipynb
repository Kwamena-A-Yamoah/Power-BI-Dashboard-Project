{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A POWER BI DASHBOARD PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Team: Team Namibia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement:\n",
    "Team Namibia has been assigned to design and deliver an end-to-end business intelligence solution. Our client has collected transactional data for the year 2019 but hasnâ€™t been able to put it to good use. The client hopes we can analyze the data and put together a report to help them find opportunities to drive more sales and work more efficiently. \n",
    "\n",
    "#### Objective\n",
    "In this analysis, we aim to:\n",
    "- Identify key trends and patterns in the 2019 transactional data.\n",
    "- Analyze sales performance and uncover opportunities to drive more sales.\n",
    "- Evaluate product performance and categorize products based on price levels.\n",
    "- Identify cities with the highest product deliveries.\n",
    "- Identify effective working processes\n",
    "\n",
    "#### Analytical Questions\n",
    "1. How much money did we make this year? \n",
    "   \n",
    "2. Can we identify any seasonality in the sales? \n",
    "\n",
    "3. What are our best and worst-selling products?\n",
    "\n",
    "4. How do sales compare to previous months or weeks?\n",
    "\n",
    "5. Which cities are our products delivered to most?\n",
    "\n",
    "6. How do product categories compare in revenue generated and quantities ordered?\n",
    "\n",
    "7. You are required to show additional details from your findings in your data. \n",
    "\n",
    "- NB: Products with unit prices above $99.99 should be labeled high-level products otherwise they should be basic level.\n",
    "\n",
    "#### Hypothesis\n",
    "- Null Hypothesis (H0): There are no significant differences in Amount amongst the group(columns) of factors being tested.\n",
    "- Alternative Hypothesis (H1): There are significant differences in Amount amongst the group(columns) of factors being tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales data was collected for each month in the entire year of 2019. \n",
    "The data for the first half of the year (January to June) was collected in Excel and saved as CSV files before management decided to use databases to store their data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyodbc library to handle ODBC database connections\n",
    "# Import the dotenv function to load environment variables from a .env file\n",
    "import pyodbc \n",
    "from dotenv import dotenv_values    \n",
    "\n",
    "# Import the warnings library to handle warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')       \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85625, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r'C:\\Users\\Pc\\Desktop\\Data analysis\\Azubi Africa\\Career Accelerator\\Capstone\\Power-BI-Dashboard-Project\\Data\\first_half_year'\n",
    "\n",
    "# Get a list of all CSV files\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Load all CSV files and concatenate them into a single DataFrame\n",
    "first_half = pd.concat([pd.read_csv(file) for file in csv_files], ignore_index=True)\n",
    "\n",
    "first_half.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Establishing a connection to the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DRIVER={SQL Server};SERVER=dap-projects-database.database.windows.net;DATABASE=dapDB;UID=capstone;PWD=Z7x@8pM$2w;MARS_Connection=yes;MinProtocolVersion=TLSv1.2;'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the .env file\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "username = environment_variables.get(\"UID\")\n",
    "password = environment_variables.get(\"PWD\")\n",
    "\n",
    "# Create the connection string using the retrieved credentials\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password};MARS_Connection=yes;MinProtocolVersion=TLSv1.2;\"\n",
    "connection_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TABLE_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sales_July_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sales_August_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sales_September_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sales_October_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sales_November_2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sales_December_2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             TABLE_NAME\n",
       "0       Sales_July_2019\n",
       "1     Sales_August_2019\n",
       "2  Sales_September_2019\n",
       "3    Sales_October_2019\n",
       "4   Sales_November_2019\n",
       "5   Sales_December_2019"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Establish a connection to the database using the connection string\n",
    "connection = pyodbc.connect(connection_string) \n",
    "\n",
    "# Define the SQL query to list all tables (for SQL Server)\n",
    "query = \"\"\"\n",
    "SELECT TABLE_NAME \n",
    "FROM INFORMATION_SCHEMA.TABLES \n",
    "WHERE TABLE_TYPE = 'BASE TABLE';\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query and fetch the result into a pandas DataFrame using the established database connection\n",
    "tables = pd.read_sql(query, connection)\n",
    "\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "('01000', '[01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionRead (recv()). (10054) (SQLGetData); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation. (11); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionRead (recv()). (10054); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation. (11)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m query6 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect * from Sales_December_2019\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Execute the SQL query and fetch the result\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m july_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m august_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query2, connection)\n\u001b[0;32m     12\u001b[0m september_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query3, connection)\n",
      "File \u001b[1;32mc:\\Users\\Pc\\Desktop\\Data analysis\\Azubi Africa\\Career Accelerator\\Capstone\\Power-BI-Dashboard-Project\\venv\\lib\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    712\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32mc:\\Users\\Pc\\Desktop\\Data analysis\\Azubi Africa\\Career Accelerator\\Capstone\\Power-BI-Dashboard-Project\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2753\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_iterator(\n\u001b[0;32m   2743\u001b[0m         cursor,\n\u001b[0;32m   2744\u001b[0m         chunksize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2750\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   2751\u001b[0m     )\n\u001b[0;32m   2752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2753\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetchall_as_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2754\u001b[0m     cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   2756\u001b[0m     frame \u001b[38;5;241m=\u001b[39m _wrap_result(\n\u001b[0;32m   2757\u001b[0m         data,\n\u001b[0;32m   2758\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2763\u001b[0m         dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   2764\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Pc\\Desktop\\Data analysis\\Azubi Africa\\Career Accelerator\\Capstone\\Power-BI-Dashboard-Project\\venv\\lib\\site-packages\\pandas\\io\\sql.py:2768\u001b[0m, in \u001b[0;36mSQLiteDatabase._fetchall_as_list\u001b[1;34m(self, cur)\u001b[0m\n\u001b[0;32m   2767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fetchall_as_list\u001b[39m(\u001b[38;5;28mself\u001b[39m, cur):\n\u001b[1;32m-> 2768\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetchall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   2770\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(result)\n",
      "\u001b[1;31mError\u001b[0m: ('01000', '[01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionRead (recv()). (10054) (SQLGetData); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation. (11); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionRead (recv()). (10054); [01000] [Microsoft][ODBC SQL Server Driver][DBNETLIB]General network error. Check your network documentation. (11)')"
     ]
    }
   ],
   "source": [
    "# Connection to each of the table\n",
    "query1 = \"Select * from Sales_July_2019\"\n",
    "query2 = \"Select * from Sales_August_2019\"\n",
    "query3 = \"Select * from Sales_September_2019\"\n",
    "query4 = \"Select * from Sales_October_2019\"\n",
    "query5 = \"Select * from Sales_November_2019\"\n",
    "query6 = \"Select * from Sales_December_2019\"\n",
    "\n",
    "# Execute the SQL query and fetch the result\n",
    "july_df = pd.read_sql(query1, connection)\n",
    "august_df = pd.read_sql(query2, connection)\n",
    "september_df = pd.read_sql(query3, connection)\n",
    "october_df = pd.read_sql(query4, connection)\n",
    "november_df = pd.read_sql(query5, connection)\n",
    "december_df = pd.read_sql(query6, connection)\n",
    "\n",
    "# Define the dictionary with DataFrames\n",
    "half_2 = {\n",
    "    'july': july_df,\n",
    "    'august': august_df,\n",
    "    'september': september_df,\n",
    "    'october': october_df,\n",
    "    'november': november_df,\n",
    "    'december': december_df\n",
    "}\n",
    "\n",
    "# Iterate over the dictionary\n",
    "for month, df in half_2.items():\n",
    "    print(f'{month}')\n",
    "    print(df.columns)\n",
    "    print(df.shape)\n",
    "    print('=' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second half of the year merged data: (101225, 6)\n"
     ]
    }
   ],
   "source": [
    "# Merge all DataFrames into one\n",
    "second_half = pd.concat(half_2, ignore_index=True)\n",
    "print('Second half of the year merged data:', second_half.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Merging both halfs into 2019_df\n",
    "# Merged_df = pd.concat([first_half, second_half], axis=0)\n",
    "\n",
    "# print('1st half year data:', first_half.shape[0])\n",
    "# print('2st half year data:', second_half.shape[0])\n",
    "# print('Total year 2019 data:', Merged_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('1st half year data:', first_half.columen)\n",
    "# print('2st half year data:', second_half.shape[0])\n",
    "# print('Total year 2019 data:', Merged_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85625, 6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_half.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101225, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_half.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 85625 entries, 0 to 85624\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   Order ID          85380 non-null  object\n",
      " 1   Product           85380 non-null  object\n",
      " 2   Quantity Ordered  85380 non-null  object\n",
      " 3   Price Each        85380 non-null  object\n",
      " 4   Order Date        85380 non-null  object\n",
      " 5   Purchase Address  85380 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 3.9+ MB\n"
     ]
    }
   ],
   "source": [
    "first_half.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101225 entries, 0 to 101224\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   Order_ID          100730 non-null  float64\n",
      " 1   Product           100925 non-null  object \n",
      " 2   Quantity_Ordered  100730 non-null  float64\n",
      " 3   Price_Each        100730 non-null  float64\n",
      " 4   Order_Date        100730 non-null  object \n",
      " 5   Purchase_Address  100925 non-null  object \n",
      "dtypes: float64(3), object(3)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "second_half.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            0\n",
       "Product             0\n",
       "Quantity Ordered    0\n",
       "Price Each          0\n",
       "Order Date          0\n",
       "Purchase Address    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_half = first_half.dropna()\n",
    "first_half.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order_ID            0\n",
       "Product             0\n",
       "Quantity_Ordered    0\n",
       "Price_Each          0\n",
       "Order_Date          0\n",
       "Purchase_Address    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_half = second_half.dropna()\n",
    "second_half.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to subtract 2000 from the year part of the date string\n",
    "# def adjust_year(date_str):\n",
    "#     date_parts = date_str.split('-')\n",
    "#     year = int(date_parts[0])\n",
    "#     adjusted_year = year - 2000\n",
    "#     return f\"{adjusted_year}-{date_parts[1]}-{date_parts[2]} {date_str[10:]}\"\n",
    "\n",
    "# # Apply the function to adjust the year\n",
    "# second_half['Adjusted_Order_Date'] = second_half['Order_Date'].apply(adjust_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        19-04-2019 08:46\n",
       "2        07-04-2019 22:30\n",
       "3        12-04-2019 14:38\n",
       "4        12-04-2019 14:38\n",
       "5        30-04-2019 09:27\n",
       "               ...       \n",
       "85620    08-05-2019 19:15\n",
       "85621    24-05-2019 22:02\n",
       "85622    24-05-2019 17:44\n",
       "85623    04-05-2019 12:46\n",
       "85624    18-05-2019 23:07\n",
       "Name: Order Date, Length: 85380, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'Order Date' to datetime\n",
    "first_half['Order Date'] = pd.to_datetime(first_half['Order Date'], format='%d-%m/%y %H:%M', errors='coerce')\n",
    "\n",
    "# Format the date as day-month-year\n",
    "first_half['Order Date'] = first_half['Order Date'].dt.strftime('%d-%m-%Y %H:%M')\n",
    "\n",
    "first_half['Order Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2019-04-19 08:46:00\n",
       "1                        NaT\n",
       "2        2019-04-07 22:30:00\n",
       "3        2019-04-12 14:38:00\n",
       "4        2019-04-12 14:38:00\n",
       "                 ...        \n",
       "101220                   NaT\n",
       "101221                   NaT\n",
       "101222                   NaT\n",
       "101223                   NaT\n",
       "101224                   NaT\n",
       "Name: Order_Date, Length: 100730, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to subtract 2000 from the year part of the date string\n",
    "def adjust_year(date_str):\n",
    "    date_parts = date_str.split('-')\n",
    "    \n",
    "    wrong_day = date_parts[0]\n",
    "    adjusted_day = wrong_day[-2:]\n",
    "\n",
    "    month = date_parts[1]\n",
    "\n",
    "    actual_year = date_parts[2]\n",
    "\n",
    "    time = date_str[10:]\n",
    "    return f\"{adjusted_day }-{month}-{actual_year} {time}\"\n",
    " \n",
    "# Apply the function to adjust the year\n",
    "second_half['Order_Date'] = second_half['Order_Date'].apply(adjust_year)\n",
    "\n",
    "# # Convert 'Order Date' to datetime\n",
    "# second_half['Order_Date'] = pd.to_datetime(first_half['Order Date'], errors='coerce')\n",
    "\n",
    "second_half['Order_Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'second_half' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msecond_half\u001b[49m[second_half[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrder_Date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'second_half' is not defined"
     ]
    }
   ],
   "source": [
    "second_half[second_half['Order_Date']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
